# Inventory for an 3-node master cluster, to be used for ex with config "cluster_3M"

# IDEA: Adding localhost to the inventory, so that it is part of "groups.all" (and it also gets important "set_fact"  )
# but only works when avoiding ssh, via ansible_connection=local
# Note: Might need to comment out for the "teardown" tags, except if in cluster playbook no "hosts: all" is used
[local]
localhost  ansible_connection=local


[master1]
eval-cdp-public0.internal.cloudapp.net host_template=Master1 ansible_host=52.168.145.76

[master2]
eval-cdp-public1.internal.cloudapp.net host_template=Master2 ansible_host=40.71.126.110

[master3]
eval-cdp-public2.internal.cloudapp.net host_template=Master3 ansible_host=40.71.162.117

[cloudera_manager:children]
master1

[edge:children]
master1

[cluster_master_nodes:children]
master1
master2
master3

[cluster_worker_nodes]
eval-cdp-public3.internal.cloudapp.net host_template=Workers ansible_host=40.71.160.58
#eval-cdp-public4.internal.cloudapp.net host_template=Workers ansible_host=__IP__
#eval-cdp-public5.internal.cloudapp.net host_template=Workers ansible_host=__IP__

[cluster:children]
#cluster_edge_nodes
cluster_master_nodes
cluster_worker_nodes


[db_server:children]
cloudera_manager

[deployment:children]
cluster
db_server

# Note: TF deploys our OS users incl. ssh-pub key, so no more need to set the "ansible_user"..
#[deployment:vars]
# Ansible will defer to the running SSH Agent for relevant keys
# Set the following to hardcode the SSH private key for the instances
# ansible_ssh_private_key_file=~/.ssh/mykey.pem  
#ansible_user=centos
#ansible_user=adminuser
